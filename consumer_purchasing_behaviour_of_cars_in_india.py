# -*- coding: utf-8 -*-
"""Consumer Purchasing Behaviour of Cars in India.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQ8Ff3-pYSunot7538oBy1MAbJKXDD-f

**Importing Libraries**
"""

!pip install bioinfokit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing
from bioinfokit.visuz import cluster
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

"""**Loading the Dataset**"""

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('Indian automobile.csv')

df

pd.pandas.set_option('Display.max_columns',None)
pd.pandas.set_option('Display.max_rows',None)

"""**Basic Information about the Dataset**"""

# first five record of the dataset
df.head()

# last five record of the dataset
df.tail()

# shape of the dataset
df.shape

# features of the dataset
df.columns

df.info()

df.dtypes

# summary  statistics of the dataset
df.describe()

# summary statistics of the categorical features
df.describe(include='object')

df['Profession'].value_counts()

df['Marrital Status'].value_counts()

df['Wife Working'].value_counts()

df['Education'].value_counts()

df['Make'].value_counts()

"""**Data Cleaning**"""

# checking null values
df.isnull().sum()

# checking duplicate values
df.duplicated().sum()

df[df['Wife Working'] == 'm']

# Deleting inconsistant value
df.drop(index=11,axis=0,inplace=True)

df.boxplot()

# removing outlier
df = df[df['Total Salary']<4700000]

df.boxplot()

"""**Univariate Analysis**"""

sns.set_palette(["cyan"])
fig,([ax0,ax1],[ax2,ax3],[ax4,ax5]) = plt.subplots(nrows=3,ncols=2,figsize=(20,15))
sns.histplot(x=df['Age'],ax=ax0)
sns.histplot(x=df['No of Dependents'],ax=ax1)
sns.histplot(x=df['Salary'],ax=ax2)
sns.histplot(x=df['Wife Salary'],ax=ax3)
sns.histplot(x=df['Total Salary'],ax=ax4)
sns.histplot(x=df['Price'],ax=ax5)
plt.show()

sns.set_palette(["violet",  "limegreen"])
fig,([ax0,ax1,ax2],[ax3,ax4,ax5]) = plt.subplots(nrows=2,ncols=3,figsize=(22,16))
sns.countplot(x=df['Profession'],ax=ax0)
sns.countplot(x=df['Marrital Status'],ax=ax1)
sns.countplot(x=df['Education'],ax=ax2)
sns.countplot(x=df['Personal loan'],ax=ax3)
sns.countplot(x=df['House Loan'],ax=ax4)
sns.countplot(x=df['Wife Working'],ax=ax5)

"""***Observation***:
* Major part of consumers are salarised and married. Also do not have any personal loan and house loan.
* Consumers with post graduate are more visible than graduate in the dataset.

"""

sns.countplot(x=df['Make'])

"""***Observation***:
* Major part of consumers purchased car with brand baleno, suv and creata.

**Bivariate Analysis**
"""

sns.pairplot(data=df)

sns.heatmap(df.corr(), annot=True, cmap='coolwarm')

"""***Observation***:
* correlation found between : price and no of dependents,salary and total salary, salary and age, price and total salary, wife salary and total salary, wife salary and price


"""

df.groupby(['Profession'])['Salary','Wife Salary','Price'].mean()

"""***Observation***:
* Generally salary, wife salary and price of salarised professional consumer are more than business professional consumer.
"""

df.groupby(['Marrital Status'])['Salary','Price'].mean()

"""***Observation***:
* Generally salary and price of married consumer are more than single consumer.

**Data Pre-processing**
"""

df.info()

df['Profession'] = df['Profession'].replace(['Salaried','Business'],[1,0])
df['Marrital Status'] = df['Marrital Status'].replace(['Married','Single'],[1,0])
df['Education'] = df['Education'].replace(['Post Graduate','Graduate'],[1,0])
df['Personal loan'] = df['Personal loan'].replace(['Yes','No'],[1,0])
df['House Loan'] = df['House Loan'].replace(['Yes','No'],[1,0])
df['Wife Working'] = df['Wife Working'].replace(['Yes','No'],[1,0])
df['Make'] = df['Make'].replace(['Baleno','SUV','Creata','i20','Ciaz','City','Duster','Verna','Luxuray'],[0,1,2,3,4,5,6,7,8])

df.head()

plt.figure(figsize=(10,8))
sns.heatmap(df.corr(),annot=True,cmap='coolwarm')

"""**Extracting Segments**

**Applying Principal Component Analysis**
"""

# scaling the dataset
scaled_data = preprocessing.scale(df)

from sklearn.decomposition import PCA

pca = PCA(n_components=13)
pc = pca.fit_transform(scaled_data)
names = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11','pc12','pc13']
pf = pd.DataFrame(data = pc, columns = names)
pf.head()

loadings = pca.components_
num_pc = pca.n_features_
pc_list = ["PC"+str(i) for i in list(range(1, num_pc+1))]
loadings_df = pd.DataFrame.from_dict(dict(zip(pc_list, loadings)))
loadings_df['feature'] = df.columns.values
loadings_df = loadings_df.set_index('feature')
loadings_df

"""**Interpretation** :
* The column PC1 indicates how the first principal component is composed of the original variables. For the first principal component total **Salary**, **Wife Salary** and **Age** are important variables. For the second principal component **Personal loan**, **House Loan** and **No of Dependents** are important variables. For the third principal component **Wife Working** and Salary are important variables.
"""

# correlation matrix plot of principal components
plt.rcParams['figure.figsize'] = (15,12)
ax = sns.heatmap(loadings_df, annot=True, cmap='Purples')
plt.show()

# get PC scores
pca_scores = PCA().fit_transform(scaled_data)

# get 2D biplot
cluster.biplot(cscore=pca_scores, loadings=loadings, labels=df.columns.values,
               var1=round(pca.explained_variance_ratio_[0]*100, 2),
    var2=round(pca.explained_variance_ratio_[1]*100, 2),show=True,dim=(10,5))

"""**Interpretation**:
*  From the above figure, we can say that **Persnal loan**, **House** **Loan** , **No of Dependents**  are quite unique. We can be interpreted as : attributes **Age**, **Marrital Status** , **Salary** point in the same direction, attributes **Total Salary**, **Make**, **Education**, **Profession** point in the same direction and **Price**, **Wife Salary**, Wife **Working** point in the same direction.

**Applying the Elbow Method**
"""

# using k-means clustering analysis for extracting segments
model = KMeans()
plt.figure(figsize=(10,8))
visualizer = KElbowVisualizer(model, k=(1,13)).fit(pf)
visualizer.show();

"""**Applying Clustering Algorithms**

**K-means Clustering**
"""

# k-means clustering
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(pf)
df['cluster_num'] = kmeans.labels_ #adding to df
# label assigned for each data point
print ('Labels:', kmeans.labels_)
# gives within-cluster sum of squares (WCSS)
print ('WCSS:', kmeans.inertia_)
# number of iterations that k-means algorithm runs to get a minimum within-cluster sum of squares
print('No. of iterations: ', kmeans.n_iter_)
# location of the centroids on each cluster
print('Cluster centroids: ', kmeans.cluster_centers_)
# checking each cluster size
print('Cluster size: ', Counter(kmeans.labels_))

# cluster visualization
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(pf)
plt.figure(figsize=(10,8))
sns.scatterplot(data=pf, x="pc1", y="pc2", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()

plt.figure(figsize=(10,8))
sns.countplot(x=df['cluster_num'])

"""**Observation** :
* Major part of the customer belongs to cluster 0 and cluster 2.



"""

sns.set_palette('coolwarm')
for i in df.drop(['cluster_num'],axis=1):
  grid = sns.FacetGrid(df,height=4, col='cluster_num')
  grid = grid.map(sns.histplot,i,bins=30)
plt.savefig('count6.png')
plt.show()

"""**Observation** :
* In cluster 3, Salaried and Business professional are almost same. In case of other cluster, salaried professional are more than Business professional.
* Cluster 0, Cluster 1 and Cluster 2 do not contain customer with 'Single' Marrital Status. In case of Cluster 3, most of the customers are single.
* In Cluster 1, Cluster 2 and Cluster 3, all customers are dependent.
* In Cluster 0, most of the customers do not have personal loan. In case of the Cluster 2, customers, have personal loan and do not have personal loan, are almost same.
* In Cluster 3 , customers do not have any house loan.
* In Cluster 0 and Cluster 1, all customer's wife engage with work. In case of cluster 2, all customer's wife do not engage with work.

***Selecting Target Segments***
"""

price = df.groupby('cluster_num')['Price'].mean()
price = price.to_frame().reset_index()
price

df['cluster_num'] = kmeans.labels_
crosstab = pd.crosstab(df['cluster_num'], df['Make'])
crosstab

df['cluster_num'] = kmeans.labels_
crosstab = pd.crosstab(df['cluster_num'], df['Price'])
crosstab

Make = df.groupby('cluster_num')['Make'].mean()
Make = Make.to_frame().reset_index()
Make

df_ = df[['Price','Make','cluster_num']]
sns.pairplot(data=df_,hue='cluster_num')

"""**Hierarchical Clustering**"""

import scipy.cluster.hierarchy as shc

plt.figure(figsize=(12, 10))
plt.title("Customer Dendograms")
dend = shc.dendrogram(shc.linkage(pf, method='ward'))
plt.axhline(y = 10, color = 'r', linestyle = '-')
plt.show()

"""**Observation** :   

*  From Dendogram we observe 4 clusters.
"""

from sklearn.cluster import AgglomerativeClustering

cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')

HierarchicalClustering = cluster.fit_predict(pf)

plt.figure(figsize=(8,6))
sns.countplot(x=HierarchicalClustering)

"""**Observation** :

* Major part of consumer belongs to Cluster 0 and Cluster 2.
"""

plt.figure(figsize=(8,6))
sns.scatterplot(data=pf, x="pc1", y="pc2", hue=cluster.labels_)

"""**Classification**"""

from sklearn.model_selection import train_test_split

x = df.drop(['cluster_num'],axis=1)
y = df['cluster_num']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42,stratify=y)
x_train.shape,y_train.shape

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

#using logisitc regression for classification
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(x_train,y_train)

## predictions
preds = clf.predict(x_test)

## performmance of the model
from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay

print(classification_report(y_test,preds))

confusion_matrix(y_test,preds)

